{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOPIQ: Top-down Image Quality Assessment Demo\n",
    "\n",
    "This notebook demonstrates how to use the TOPIQ standalone implementation for image quality assessment tasks. TOPIQ (TOP-down approach for Image Quality assessment) is a state-of-the-art model that uses a cross-scale feature attention network to evaluate image quality.\n",
    "\n",
    "## What is TOPIQ?\n",
    "\n",
    "TOPIQ is a model described in the paper [\"TOPIQ: A Top-down Approach from Semantics to Distortions for Image Quality Assessment\"](https://arxiv.org/abs/2308.03060). It provides accurate image quality assessment using a unique cross-scale feature attention network approach.\n",
    "\n",
    "The model offers multiple variants:\n",
    "- No-reference (NR) assessment - evaluates quality without needing a reference image\n",
    "- Full-reference (FR) assessment - compares a distorted image to a reference image\n",
    "- Face-specific assessment - specialized for evaluating face image quality\n",
    "- Aesthetic assessment - evaluates aesthetic quality rather than just technical quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's install any required packages if needed and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run if you need to install these packages\n",
    "# !pip install torch torchvision timm numpy Pillow requests matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Import TOPIQ model\n",
    "from topiq_model import create_topiq, load_image\n",
    "\n",
    "# Set up device (use GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Sample Images\n",
    "\n",
    "Let's download and prepare a set of sample images with varying quality and content to test our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory for sample images\n",
    "sample_dir = Path(\"sample_images\")\n",
    "sample_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Function to download images\n",
    "def download_image(url, save_path):\n",
    "    try:\n",
    "        response = requests.get(url, stream=True)\n",
    "        if response.status_code == 200:\n",
    "            with open(save_path, 'wb') as f:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "            print(f\"Downloaded {save_path}\")\n",
    "            return Image.open(save_path)\n",
    "        else:\n",
    "            print(f\"Failed to download image, status code: {response.status_code}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {url}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample image URLs - these represent different quality levels and image types\n",
    "image_urls = {\n",
    "    \"high_quality\": \"https://unsplash.com/photos/qrLV1ABFqFY/download?ixid=M3wxMjA3fDB8MXxzZWFyY2h8MTB8fGhpZ2glMjBxdWFsaXR5fGVufDB8fHx8MTcxMDgxMjUyN3ww&force=true&w=1920\",\n",
    "    \"medium_quality\": \"https://unsplash.com/photos/random/download?ixid=M3wxMjA3fDB8MXxzZWFyY2h8MTF8fG1lZGl1bSUyMHF1YWxpdHl8ZW58MHx8fHwxNzEwODEyNjYzfDA&force=true&w=1080\",\n",
    "    \"low_quality\": \"https://unsplash.com/photos/random/download?ixid=M3wxMjA3fDB8MXxzZWFyY2h8MTB8fGxvdyUyMHF1YWxpdHl8ZW58MHx8fHwxNzEwODEyNjkzfDA&force=true&w=640\",\n",
    "    \"face\": \"https://unsplash.com/photos/random/download?ixid=M3wxMjA3fDB8MXxzZWFyY2h8Mnx8ZmFjZSUyMHBvcnRyYWl0fGVufDB8fHx8MTcxMDgxMjczMXww&force=true&w=1080\",\n",
    "    \"artistic\": \"https://unsplash.com/photos/random/download?ixid=M3wxMjA3fDB8MXxzZWFyY2h8M3x8YXJ0aXN0aWMlMjBwaG90b2dyYXBoeXxlbnwwfHx8fDE3MTA4MTI3NTV8MA&force=true&w=1080\"\n",
    "}\n",
    "\n",
    "# Dictionary to store our images\n",
    "images = {}\n",
    "\n",
    "# Download images if they don't already exist\n",
    "for name, url in image_urls.items():\n",
    "    img_path = sample_dir / f\"{name}.jpg\"\n",
    "    if img_path.exists():\n",
    "        print(f\"Loading existing image: {img_path}\")\n",
    "        images[name] = Image.open(img_path)\n",
    "    else:\n",
    "        print(f\"Downloading {name} image...\")\n",
    "        img = download_image(url, img_path)\n",
    "        if img:\n",
    "            images[name] = img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the images\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for i, (name, img) in enumerate(images.items()):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(name.replace('_', ' ').title())\n",
    "    plt.axis('off')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. No-Reference Image Quality Assessment\n",
    "\n",
    "Let's start with no-reference image quality assessment, which evaluates the quality of images without requiring a reference image. This is useful for real-world scenarios where we don't have access to pristine reference images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TOPIQ No-Reference model\n",
    "print(\"Loading TOPIQ No-Reference model...\")\n",
    "model_nr = create_topiq(model_name=\"topiq_nr\", device=device)\n",
    "\n",
    "# Function to assess image quality\n",
    "def assess_quality(model, image_path):\n",
    "    \"\"\"Assess the quality of an image using a TOPIQ model\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Load and process image\n",
    "    if isinstance(image_path, Path):\n",
    "        image_path = str(image_path)\n",
    "    if isinstance(image_path, Image.Image):\n",
    "        # Convert PIL image to tensor\n",
    "        img = np.array(image_path).astype(np.float32) / 255.0\n",
    "        img = torch.from_numpy(img).permute(2, 0, 1).unsqueeze(0).to(device)\n",
    "    else:\n",
    "        img = load_image(image_path).to(device)\n",
    "    \n",
    "    # Get quality score\n",
    "    with torch.no_grad():\n",
    "        score = model(img)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    processing_time = (end_time - start_time) * 1000  # ms\n",
    "    \n",
    "    return score.item(), processing_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess quality of our sample images\n",
    "results = {}\n",
    "print(\"Assessing image quality using TOPIQ No-Reference model...\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Image':<15} | {'Score':<10} | {'Time (ms)':<10}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for name, img in images.items():\n",
    "    img_path = sample_dir / f\"{name}.jpg\"\n",
    "    score, proc_time = assess_quality(model_nr, img_path)\n",
    "    results[name] = score\n",
    "    print(f\"{name:<15} | {score:.4f}      | {proc_time:.2f}\")\n",
    "\n",
    "print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing No-Reference Quality Scores\n",
    "\n",
    "Now let's visualize the quality scores for our sample images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Create bar chart of quality scores\n",
    "names = list(results.keys())\n",
    "scores = list(results.values())\n",
    "\n",
    "plt.bar(names, scores, color='skyblue')\n",
    "plt.xlabel('Images')\n",
    "plt.ylabel('Quality Score (0-1)')\n",
    "plt.title('TOPIQ No-Reference Quality Assessment')\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Add score values on top of bars\n",
    "for i, score in enumerate(scores):\n",
    "    plt.text(i, score + 0.02, f'{score:.4f}', ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Grid with Quality Scores\n",
    "\n",
    "Let's also display the images alongside their quality scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for i, (name, img) in enumerate(images.items()):\n",
    "    score = results[name]\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"{name.replace('_', ' ').title()}\\nQuality: {score:.4f}\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Face-specific Image Quality Assessment\n",
    "\n",
    "TOPIQ provides specialized models for face image quality assessment. This is particularly useful for applications involving facial recognition, portrait photography, or facial analysis where the quality of face images is crucial.\n",
    "\n",
    "Let's test the face-specific model on our face image and compare it with the general model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the face-specific TOPIQ model\n",
    "print(\"Loading TOPIQ Face Quality Assessment model...\")\n",
    "model_face = create_topiq(model_name=\"topiq_nr-face\", device=device)\n",
    "\n",
    "# Check if we have a face image in our collection\n",
    "if 'face' in images:\n",
    "    face_img_path = sample_dir / \"face.jpg\"\n",
    "    \n",
    "    # Get quality scores from both models\n",
    "    face_score, face_time = assess_quality(model_face, face_img_path)\n",
    "    general_score = results['face']  # We already computed this with the general model\n",
    "    \n",
    "    print(f\"Face-specific quality score: {face_score:.4f} (processed in {face_time:.2f} ms)\")\n",
    "    print(f\"General quality score:      {general_score:.4f}\")\n",
    "    \n",
    "    # Display the face image with both scores\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(images['face'])\n",
    "    plt.title(f\"Face Image Quality Assessment\\n\"\n",
    "              f\"Face-specific score: {face_score:.4f}\\n\"\n",
    "              f\"General score: {general_score:.4f}\")\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No face image found in the dataset. Please add a face image to test this model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Face Models\n",
    "\n",
    "TOPIQ offers multiple variants of face quality assessment models. Let's compare them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different face models if available\n",
    "if 'face' in images:\n",
    "    face_img_path = sample_dir / \"face.jpg\"\n",
    "    \n",
    "    face_models = {\n",
    "        \"topiq_nr-face\": create_topiq(model_name=\"topiq_nr-face\", device=device),\n",
    "        \"topiq_nr-face-v1\": create_topiq(model_name=\"topiq_nr-face-v1\", device=device),\n",
    "    }\n",
    "    \n",
    "    # Evaluate with each model\n",
    "    face_results = {}\n",
    "    print(\"\\nComparing different face quality assessment models:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'Model':<20} | {'Score':<10} | {'Time (ms)':<10}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for model_name, model in face_models.items():\n",
    "        score, proc_time = assess_quality(model, face_img_path)\n",
    "        face_results[model_name] = score\n",
    "        print(f\"{model_name:<20} | {score:.4f}      | {proc_time:.2f}\")\n",
    "        \n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Add the general model for comparison\n",
    "    face_results[\"General NR\"] = results['face']\n",
    "    \n",
    "    # Create a bar chart to compare models\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(face_results.keys(), face_results.values(), color=['blue', 'green', 'orange'])\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Quality Score (0-1)')\n",
    "    plt.title('Face Quality Assessment Model Comparison')\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    # Add score values on top of bars\n",
    "    for i, (model, score) in enumerate(face_results.items()):\n",
    "        plt.text(i, score + 0.02, f'{score:.4f}', ha='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No face image available for comparison.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Aesthetic Image Quality Assessment\n",
    "\n",
    "In addition to technical quality assessment, TOPIQ can also evaluate the aesthetic quality of images. Aesthetic quality refers to how visually pleasing or attractive an image is, rather than just its technical aspects.\n",
    "\n",
    "The aesthetic model (topiq_iaa) returns scores on a scale from 1 to 10, where higher scores indicate better aesthetic quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TOPIQ aesthetic model\n",
    "print(\"Loading TOPIQ Aesthetic Assessment model...\")\n",
    "model_iaa = create_topiq(model_name=\"topiq_iaa\", device=device)\n",
    "\n",
    "# Assess aesthetic quality of our sample images\n",
    "aesthetic_results = {}\n",
    "print(\"Assessing aesthetic quality using TOPIQ IAA model...\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Image':<15} | {'Aesthetic Score':<15} | {'Time (ms)':<10}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for name, img in images.items():\n",
    "    img_path = sample_dir / f\"{name}.jpg\"\n",
    "    score, proc_time = assess_quality(model_iaa, img_path)\n",
    "    aesthetic_results[name] = score\n",
    "    print(f\"{name:<15} | {score:.2f}/10          | {proc_time:.2f}\")\n",
    "\n",
    "print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize aesthetic scores\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "names = list(aesthetic_results.keys())\n",
    "scores = list(aesthetic_results.values())\n",
    "\n",
    "plt.bar(names, scores, color='purple')\n",
    "plt.xlabel('Images')\n",
    "plt.ylabel('Aesthetic Score (1-10)')\n",
    "plt.title('TOPIQ Aesthetic Quality Assessment')\n",
    "plt.ylim(1, 10)\n",
    "\n",
    "# Add score values on top of bars\n",
    "for i, score in enumerate(scores):\n",
    "    plt.text(i, score + 0.2, f'{score:.2f}', ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Technical vs. Aesthetic Quality\n",
    "\n",
    "Let's compare technical quality scores (from the no-reference model) with aesthetic quality scores for our sample images. This can show interesting differences between technically good images and aesthetically pleasing ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comparison between technical and aesthetic scores\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Scale aesthetic scores to 0-1 for comparison\n",
    "scaled_aesthetic_scores = [score / 10 for score in aesthetic_results.values()]\n",
    "\n",
    "x = np.arange(len(names))\n",
    "width = 0.35\n",
    "\n",
    "# Create grouped bar chart\n",
    "plt.bar(x - width/2, list(results.values()), width, label='Technical Quality (0-1)', color='skyblue')\n",
    "plt.bar(x + width/2, scaled_aesthetic_scores, width, label='Aesthetic Quality (scaled to 0-1)', color='purple')\n",
    "\n",
    "plt.xlabel('Images')\n",
    "plt.ylabel('Score (0-1)')\n",
    "plt.title('Technical vs. Aesthetic Quality Comparison')\n",
    "plt.xticks(x, names)\n",
    "plt.legend()\n",
    "\n",
    "# Add score values on top of bars\n",
    "for i, (tech_score, aes_score) in enumerate(zip(results.values(), aesthetic_results.values())):\n",
    "    plt.text(i - width/2, tech_score + 0.02, f'{tech_score:.2f}', ha='center', va='bottom')\n",
    "    plt.text(i + width/2, scaled_aesthetic_scores[i] + 0.02, f'{aes_score:.1f}/10', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying Images with Technical and Aesthetic Scores\n",
    "\n",
    "Let's visualize our images with both their technical and aesthetic quality scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 12))\n",
    "\n",
    "for i, (name, img) in enumerate(images.items()):\n",
    "    tech_score = results[name]\n",
    "    aes_score = aesthetic_results[name]\n",
    "    \n",
    "    plt.subplot(2, 3, i+1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"{name.replace('_', ' ').title()}\\nTechnical: {tech_score:.2f}\\nAesthetic: {aes_score:.2f}/10\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Full-Reference Image Quality Assessment\n",
    "\n",
    "Full-Reference Image Quality Assessment (FR-IQA) measures the quality of an image by comparing it with a reference (undistorted) version. This approach can provide precise measurements of how much a processed or distorted image deviates from its original version.\n",
    "\n",
    "Let's first create some distorted versions of our high-quality image to demonstrate FR-IQA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create distorted versions of our high-quality image\n",
    "def create_distorted_images(reference_image_path, save_dir):\n",
    "    # Open reference image\n",
    "    ref_img = Image.open(reference_image_path)\n",
    "    \n",
    "    # Create distortions with different levels of JPEG compression\n",
    "    compression_levels = [90, 60, 30, 10]  # JPEG quality levels\n",
    "    distorted_imgs = {}\n",
    "    \n",
    "    for quality in compression_levels:\n",
    "        dist_path = save_dir / f\"distorted_q{quality}.jpg\"\n",
    "        if not dist_path.exists():\n",
    "            print(f\"Creating JPEG distortion with quality {quality}...\")\n",
    "            ref_img.save(dist_path, format=\"JPEG\", quality=quality)\n",
    "        \n",
    "        dist_img = Image.open(dist_path)\n",
    "        distorted_imgs[f\"JPEG Q{quality}\"] = dist_img\n",
    "    \n",
    "    return distorted_imgs\n",
    "\n",
    "# Select a reference image (using high_quality)\n",
    "reference_img_path = sample_dir / \"high_quality.jpg\"\n",
    "if not reference_img_path.exists() and 'high_quality' in images:\n",
    "    reference_img_path = sample_dir / \"high_quality.jpg\"\n",
    "\n",
    "# Create distorted images\n",
    "if reference_img_path.exists():\n",
    "    print(f\"Using {reference_img_path} as the reference image\")\n",
    "    distorted_images = create_distorted_images(reference_img_path, sample_dir)\n",
    "else:\n",
    "    print(\"No suitable reference image found. Please ensure a high-quality image is available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display reference and distorted images\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Display reference image\n",
    "plt.subplot(2, 3, 1)\n",
    "ref_img = Image.open(reference_img_path)\n",
    "plt.imshow(ref_img)\n",
    "plt.title(\"Reference Image\")\n",
    "plt.axis('off')\n",
    "\n",
    "# Display distorted images\n",
    "for i, (name, img) in enumerate(distorted_images.items()):\n",
    "    plt.subplot(2, 3, i+2)\n",
    "    plt.imshow(img)\n",
    "    plt.title(name)\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Full-Reference Quality\n",
    "\n",
    "Now let's use TOPIQ's Full-Reference model to evaluate how closely each distorted image resembles the reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TOPIQ FR model\n",
    "print(\"Loading TOPIQ Full-Reference model...\")\n",
    "model_fr = create_topiq(model_name=\"topiq_fr\", device=device)\n",
    "\n",
    "# Function for FR assessment\n",
    "def assess_fr_quality(model, distorted_img_path, reference_img_path):\n",
    "    \"\"\"Assess the quality of a distorted image compared to a reference\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Load images\n",
    "    dist_img = load_image(distorted_img_path).to(device)\n",
    "    ref_img = load_image(reference_img_path).to(device)\n",
    "    \n",
    "    # Get quality score\n",
    "    with torch.no_grad():\n",
    "        score = model(dist_img, ref_img)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    processing_time = (end_time - start_time) * 1000  # ms\n",
    "    \n",
    "    return score.item(), processing_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate each distorted image\n",
    "fr_results = {}\n",
    "print(\"Evaluating distorted images using TOPIQ FR model...\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Distortion':<15} | {'FR Score':<10} | {'Time (ms)':<10}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for name, img in distorted_images.items():\n",
    "    quality = name.split(\"Q\")[1]\n",
    "    dist_path = sample_dir / f\"distorted_q{quality}.jpg\"\n",
    "    score, proc_time = assess_fr_quality(model_fr, dist_path, reference_img_path)\n",
    "    fr_results[name] = score\n",
    "    print(f\"{name:<15} | {score:.4f}      | {proc_time:.2f}\")\n",
    "\n",
    "print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing FR-IQA Results\n",
    "\n",
    "Let's visualize the FR quality scores. We expect higher scores for images that are closer to the reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bar chart of FR scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "names = list(fr_results.keys())\n",
    "scores = list(fr_results.values())\n",
    "\n",
    "plt.bar(names, scores, color='orange')\n",
    "plt.xlabel('Distortion Level')\n",
    "plt.ylabel('Quality Score (0-1)')\n",
    "plt.title('TOPIQ Full-Reference Quality Assessment')\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Add score values on top of bars\n",
    "for i, score in enumerate(scores):\n",
    "    plt.text(i, score + 0.02, f'{score:.4f}', ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying Images with FR Scores\n",
    "\n",
    "Let's visualize the distorted images along with their FR quality scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Display reference image\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.imshow(ref_img)\n",
    "plt.title(\"Reference Image\")\n",
    "plt.axis('off')\n",
    "\n",
    "# Display distorted images with scores\n",
    "for i, (name, img) in enumerate(distorted_images.items()):\n",
    "    score = fr_results[name]\n",
    "    plt.subplot(2, 3, i+2)\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"{name}\\nQuality: {score:.4f}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparing Different TOPIQ Model Variants\n",
    "\n",
    "TOPIQ offers several different model variants, each specialized for different tasks or trained on different datasets. Let's load several variants and compare how they evaluate the same set of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load different TOPIQ model variants for comparison\n",
    "print(\"Loading different TOPIQ model variants...\")\n",
    "topiq_models = {\n",
    "    \"General NR\": create_topiq(model_name=\"topiq_nr\", device=device),\n",
    "    \"Face NR\": create_topiq(model_name=\"topiq_nr-face\", device=device),\n",
    "    \"FLIVE NR\": create_topiq(model_name=\"topiq_nr-flive\", device=device),\n",
    "    \"SPAQ NR\": create_topiq(model_name=\"topiq_nr-spaq\", device=device),\n",
    "    \"Aesthetic\": create_topiq(model_name=\"topiq_iaa\", device=device)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all images with all models\n",
    "model_comparison = {model_name: {} for model_name in topiq_models.keys()}\n",
    "\n",
    "print(\"Comparing TOPIQ models across all images...\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Image':<15} | {'General NR':<10} | {'Face NR':<10} | {'FLIVE NR':<10} | {'SPAQ NR':<10} | {'Aesthetic':<10}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for img_name, img in images.items():\n",
    "    img_path = sample_dir / f\"{img_name}.jpg\"\n",
    "    scores = []\n",
    "    \n",
    "    # Evaluate image with each model\n",
    "    for model_name, model in topiq_models.items():\n",
    "        score, _ = assess_quality(model, img_path)\n",
    "        model_comparison[model_name][img_name] = score\n",
    "        \n",
    "        # Format the score for display\n",
    "        if model_name == \"Aesthetic\":\n",
    "            formatted_score = f\"{score:.2f}/10\"\n",
    "        else:\n",
    "            formatted_score = f\"{score:.4f}\"\n",
    "        \n",
    "        scores.append(formatted_score)\n",
    "    \n",
    "    # Print results for this image\n",
    "    print(f\"{img_name:<15} | {scores[0]:<10} | {scores[1]:<10} | {scores[2]:<10} | {scores[3]:<10} | {scores[4]:<10}\")\n",
    "\n",
    "print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Model Comparison with Radar Chart\n",
    "\n",
    "Let's create a radar chart to better visualize how different models evaluate the same images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a radar chart for comparing models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def radar_chart(data, categories, title):\n",
    "    # Number of variables\n",
    "    N = len(categories)\n",
    "    \n",
    "    # What will be the angle of each axis in the plot\n",
    "    angles = [n / N * 2 * np.pi for n in range(N)]\n",
    "    angles += angles[:1]  # Close the loop\n",
    "    \n",
    "    # Initialize the figure\n",
    "    fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(polar=True))\n",
    "    \n",
    "    # Draw one axis per variable and add labels\n",
    "    plt.xticks(angles[:-1], categories, size=12)\n",
    "    \n",
    "    # Draw ylabels\n",
    "    ax.set_rlabel_position(0)\n",
    "    plt.yticks([0.2, 0.4, 0.6, 0.8], [\"0.2\",\"0.4\",\"0.6\",\"0.8\"], color=\"grey\", size=10)\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    # Plot data\n",
    "    for i, (model_name, values) in enumerate(data.items()):\n",
    "        # Scale aesthetic scores for comparison (1-10 to 0-1)\n",
    "        if model_name == \"Aesthetic\":\n",
    "            values = {k: v/10 for k, v in values.items()}\n",
    "            \n",
    "        # Plot values\n",
    "        values_list = [values[cat] for cat in categories]\n",
    "        values_list += values_list[:1]  # Close the loop\n",
    "        \n",
    "        ax.plot(angles, values_list, linewidth=2, linestyle='solid', label=model_name)\n",
    "        ax.fill(angles, values_list, alpha=0.1)\n",
    "    \n",
    "    # Add legend\n",
    "    plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
    "    \n",
    "    # Add title\n",
    "    plt.title(title, size=15, y=1.1)\n",
    "    \n",
    "    return fig, ax\n",
    "\n",
    "# Create the radar chart\n",
    "radar_chart(model_comparison, list(images.keys()), \"TOPIQ Model Comparison Across Images\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Characteristics and Use Cases\n",
    "\n",
    "Let's analyze the differences between these models:\n",
    "\n",
    "- **General NR**: Balanced performance across various image types, good general-purpose model\n",
    "- **Face NR**: Optimized for face image quality evaluation, considers face-specific attributes\n",
    "- **FLIVE NR**: Trained on FLIVE dataset with more user-generated content, may better reflect user perceptions\n",
    "- **SPAQ NR**: Trained on smartphone photos, optimized for mobile photography quality assessment\n",
    "- **Aesthetic**: Evaluates artistic/aesthetic quality rather than just technical quality\n",
    "\n",
    "The ideal model to use depends on your specific application:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Batch Processing for Efficiency\n",
    "\n",
    "For applications that need to process many images, batch processing can significantly improve efficiency. Let's demonstrate how to process images in batches with TOPIQ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process images in batches\n",
    "def batch_process(model, image_paths, batch_size=4):\n",
    "    \"\"\"Process multiple images in batches for efficiency\"\"\"\n",
    "    all_scores = []\n",
    "    total_images = len(image_paths)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in range(0, total_images, batch_size):\n",
    "        batch_paths = image_paths[i:min(i+batch_size, total_images)]\n",
    "        batch_tensors = []\n",
    "        \n",
    "        # Load each image in the batch\n",
    "        for path in batch_paths:\n",
    "            tensor = load_image(path).to(device)\n",
    "            batch_tensors.append(tensor)\n",
    "        \n",
    "        # Stack tensors into a batch\n",
    "        batch_input = torch.cat(batch_tensors, dim=0)\n",
    "        \n",
    "        # Process batch\n",
    "        with torch.no_grad():\n",
    "            batch_scores = model(batch_input)\n",
    "        \n",
    "        # Add scores to results\n",
    "        all_scores.extend(batch_scores.cpu().numpy().tolist())\n",
    "    \n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    \n",
    "    return all_scores, total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a larger test set by duplicating our images\n",
    "test_paths = [str(sample_dir / f\"{name}.jpg\") for name in images.keys()] * 5  # Repeat each image 5 times\n",
    "print(f\"Created test batch with {len(test_paths)} images\")\n",
    "\n",
    "# Test different batch sizes\n",
    "model = topiq_models[\"General NR\"]\n",
    "batch_sizes = [1, 2, 4, 8, 16]  # Test different batch sizes\n",
    "batch_results = {}\n",
    "\n",
    "for bs in batch_sizes:\n",
    "    if bs > len(test_paths):\n",
    "        continue\n",
    "    \n",
    "    print(f\"Processing with batch size {bs}...\")\n",
    "    scores, total_time = batch_process(model, test_paths, batch_size=bs)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    images_per_second = len(test_paths) / total_time\n",
    "    ms_per_image = 1000 * total_time / len(test_paths)\n",
    "    \n",
    "    batch_results[bs] = {\n",
    "        \"total_time\": total_time,\n",
    "        \"images_per_second\": images_per_second,\n",
    "        \"ms_per_image\": ms_per_image\n",
    "    }\n",
    "    \n",
    "    print(f\"  Processed {len(test_paths)} images in {total_time:.2f} seconds\")\n",
    "    print(f\"  Speed: {images_per_second:.2f} images/sec ({ms_per_image:.2f} ms/image)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize batch processing efficiency\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot processing speed (images per second)\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(\n",
    "    [f\"Batch {bs}\" for bs in batch_results.keys()], \n",
    "    [data[\"images_per_second\"] for data in batch_results.values()],\n",
    "    color=\"green\"\n",
    ")\n",
    "plt.ylabel(\"Images per second\")\n",
    "plt.title(\"Processing Speed by Batch Size\")\n",
    "\n",
    "# Plot time per image\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(\n",
    "    [f\"Batch {bs}\" for bs in batch_results.keys()], \n",
    "    [data[\"ms_per_image\"] for data in batch_results.values()],\n",
    "    color=\"blue\"\n",
    ")\n",
    "plt.ylabel(\"Time per image (ms)\")\n",
    "plt.title(\"Processing Time by Batch Size\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've explored the TOPIQ model's various capabilities:\n",
    "\n",
    "1. **No-Reference Quality Assessment**: Evaluating image quality without a reference\n",
    "2. **Face-specific Quality Assessment**: Specialized evaluation for face images\n",
    "3. **Aesthetic Quality Assessment**: Measuring the artistic/visual appeal of images\n",
    "4. **Full-Reference Quality Assessment**: Comparing distorted images to their references\n",
    "5. **Model Comparison**: Understanding the differences between model variants\n",
    "6. **Batch Processing**: Efficiently processing multiple images\n",
    "\n",
    "TOPIQ provides a versatile toolkit for image quality assessment across various use cases, from technical quality evaluation to aesthetic judgment, and from general-purpose assessment to specialized domains like facial image analysis.\n",
    "\n",
    "For more details on the TOPIQ model, refer to the [paper](https://arxiv.org/abs/2308.03060) and the [original implementation](https://github.com/chaofengc/IQA-PyTorch)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
